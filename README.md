# bd-a1
ASSIGNMENT REQUIREMENTS:
- Start by creating a directory on your local machine named bd-a1/.
- Download and place the dataset in the bd-a1/ directory .
- Inside the bd-a1/ directory, create a Dockerfile.
- Specify the base image as Ubuntu. 
- Install the following packages in the Dockerfile: Python3, Pandas, Numpy,
Seaborn, Matplotlib, scikit-learn, and Scipy. [
- Create a directory inside the container at /home/doc-bd-a1/.
- Move the dataset file to the container.
- Open the bash shell upon container startup. 
- Note: Install any additional modules or libraries you anticipate needing within the
container.
- Within the container's doc-bd-a1/ directory, create the following files:
- load.py: Design this file to dynamically read the dataset file by accepting the file
path as a user-provided argument.
- dpre.py: This file should perform Data Cleaning, Data Transformation, Data
Reduction, and Data Discretization steps. In each step apply minimum 2 tasks.
Nile University | Fall 2023 | CSCI461 Introduction to Big Data
Save the resulting data frame as a new CSV file named res_dpre.csv. 
- eda.py: Conduct exploratory data analysis, generating at least 3 insights without
visualizations. Save these insights as text files named eda-in-1.txt, and so on.
- vis.py: Create a single visualization and save it as vis.png.
- model.py: Implement the K-means algorithm on your data frame with the
columns you deem suitable for K-means, setting k=3. Save the number of records
in each cluster as a text file named k.txt.
- final.sh: Compose a simple bash script to copy the output files generated by
dpre.py, eda.py, vis.py, and model.py from the container to your local machine
in bd-a1/service-result/. Finally, the script should close the container. 
Notes:
● Each Python file responsible for updating the data frame should invoke the next Python
file and transmit the data frame path to it. Subsequently, read the CSV file as a data frame
and continue processing.
● To execute your project, perform the following steps:
○ After creating the Dockerfile, build it to produce an image.
○ Run the container using the generated image.
○ Inside the container, create the Python & Bash files as specified.
○ Initiate the pipeline using the command: python3 load.py <dataset-path>. 
○ The pipeline will generate several files and figures, conforming to the prescribed
outputs. These will be relocated from the container to your local machine in
bd-a1/service-result/.
○ Execute a bash script to halt/stop the container.
